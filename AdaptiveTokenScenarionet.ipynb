{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scenarionet import read_dataset_summary, read_scenario\n",
    "from metadrive.engine.asset_loader import AssetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'version', 'length', 'tracks', 'dynamic_map_states', 'map_features', 'metadata'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av2_data =  AssetLoader.file_path(\"/home/light/Documents/Thesis/preprocessed_dataset\", unix_style=False)\n",
    "dataset_summary, scenario_ids, mapping = read_dataset_summary(dataset_path=av2_data)\n",
    "\n",
    "scenario_file_name = scenario_ids[0]\n",
    "scenario = read_scenario(dataset_path=av2_data, mapping=mapping, scenario_file_name=scenario_file_name)\n",
    "scenario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class AdaptiveTokenScenarioNet:\n",
    "    def __init__(self, importance_threshold=0.5, max_tokens=512, token_length=10, use_gpu=False):\n",
    "        self.importance_threshold = importance_threshold\n",
    "        self.max_tokens = max_tokens\n",
    "        self.token_length = token_length\n",
    "        self.device = torch.device('cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu')\n",
    "        self.type_importance = {\n",
    "            'VEHICLE': 1.0,\n",
    "            'PEDESTRIAN': 0.8,\n",
    "            'BICYCLE': 0.7,\n",
    "            'STATIC': 0.5,\n",
    "            'UNKNOWN': 0.3\n",
    "        }\n",
    "        self.map_feature_importance = {\n",
    "            'LANE': 0.9,\n",
    "            'CROSSWALK': 0.7,\n",
    "            'STOP_SIGN': 0.6,\n",
    "            'TRAFFIC_LIGHT': 0.8,\n",
    "            'UNKNOWN': 0.4\n",
    "        }\n",
    "\n",
    "    def score_tracks(self, tracks):\n",
    "        importance_scores = {}\n",
    "        for track_id, track in tracks.items():\n",
    "            state = track['state']\n",
    "            valid = state['valid']\n",
    "            if not valid.any():\n",
    "                importance_scores[track_id] = 0\n",
    "                continue\n",
    "\n",
    "            valid_positions = state['position'][valid]\n",
    "            valid_velocities = state['velocity'][valid]\n",
    "            valid_headings = state['heading'][valid]\n",
    "            valid_lengths = state['length'][valid]\n",
    "            valid_widths = state['width'][valid]\n",
    "            valid_heights = state['height'][valid]\n",
    "\n",
    "            activity_score = np.linalg.norm(valid_positions, axis=1).sum()\n",
    "            velocity_score = np.linalg.norm(valid_velocities, axis=1).sum()\n",
    "            heading_score = np.abs(valid_headings).sum()\n",
    "            size_score = valid_lengths.sum() + valid_widths.sum() + valid_heights.sum()\n",
    "            type_score = self.type_importance.get(track.get('type', 'UNKNOWN'), 0.3)\n",
    "\n",
    "            importance_scores[track_id] = activity_score + velocity_score + heading_score + size_score + type_score\n",
    "        return importance_scores\n",
    "\n",
    "    def score_map_features(self, map_features):\n",
    "        importance_scores = {}\n",
    "        for feature_id, feature in map_features.items():\n",
    "            feature_type = feature.get('type', 'UNKNOWN')\n",
    "            polyline = feature.get('polyline', [])\n",
    "            if len(polyline) == 0:\n",
    "                importance_scores[feature_id] = 0\n",
    "                continue\n",
    "\n",
    "            feature_score = len(polyline) * self.map_feature_importance.get(feature_type, 0.4)\n",
    "            importance_scores[feature_id] = feature_score\n",
    "        return importance_scores\n",
    "\n",
    "    def pad_or_truncate(self, token, expected_dim):\n",
    "        if token.shape[1] != expected_dim:\n",
    "            token = token[:, :expected_dim] if token.shape[1] > expected_dim else np.pad(\n",
    "                token, ((0, 0), (0, expected_dim - token.shape[1])), mode='constant'\n",
    "            )\n",
    "        if token.shape[0] > self.token_length:\n",
    "            token = token[:self.token_length]\n",
    "        else:\n",
    "            pad_length = self.token_length - token.shape[0]\n",
    "            pad = np.zeros((pad_length, expected_dim), dtype=np.float32)\n",
    "            token = np.vstack((token, pad))\n",
    "        return token\n",
    "\n",
    "    def tokenize_tracks(self, tracks):\n",
    "        tokenized_tracks = {}\n",
    "        for track_id, track in tracks.items():\n",
    "            state = track['state']\n",
    "            valid = state['valid']\n",
    "            if not valid.any():\n",
    "                tokenized_tracks[track_id] = torch.empty((self.token_length, 9), dtype=torch.float32, device=self.device)\n",
    "                continue\n",
    "\n",
    "            valid_positions = state['position'][valid]\n",
    "            valid_velocities = state['velocity'][valid]\n",
    "            valid_headings = state['heading'][valid]\n",
    "            valid_lengths = state['length'][valid]\n",
    "            valid_widths = state['width'][valid]\n",
    "            valid_heights = state['height'][valid]\n",
    "\n",
    "            combined_token = np.column_stack((valid_positions, valid_velocities, valid_headings, valid_lengths, valid_widths, valid_heights))\n",
    "            token = self.pad_or_truncate(combined_token, expected_dim=9)\n",
    "            tokenized_tracks[track_id] = torch.tensor(token, dtype=torch.float32, device=self.device)\n",
    "        return tokenized_tracks\n",
    "\n",
    "    def tokenize_map_features(self, map_features):\n",
    "        tokenized_features = {}\n",
    "        for feature_id, feature in map_features.items():\n",
    "            polyline = feature.get('polyline', [])\n",
    "            if len(polyline) == 0:\n",
    "                tokenized_features[feature_id] = torch.empty((self.token_length, 2), dtype=torch.float32, device=self.device)\n",
    "                continue\n",
    "\n",
    "            polyline_array = np.array(polyline)\n",
    "            polyline_array = polyline_array[:, :2] if polyline_array.shape[1] > 2 else polyline_array\n",
    "            token = self.pad_or_truncate(polyline_array, expected_dim=2)\n",
    "            tokenized_features[feature_id] = torch.tensor(token, dtype=torch.float32, device=self.device)\n",
    "        return tokenized_features\n",
    "\n",
    "    def process(self, scenario):\n",
    "        tracks = scenario.get('tracks', {})\n",
    "        map_features = scenario.get('map_features', {})\n",
    "        metadata = scenario.get('metadata', {})\n",
    "\n",
    "        track_scores = self.score_tracks(tracks)\n",
    "        map_scores = self.score_map_features(map_features)\n",
    "\n",
    "        tokenized_tracks = self.tokenize_tracks(tracks)\n",
    "        tokenized_map_features = self.tokenize_map_features(map_features)\n",
    "\n",
    "        scenario['tracks'] = {\n",
    "            track_id: {\n",
    "                **track,\n",
    "                'importance_score': track_scores[track_id],\n",
    "                'token': tokenized_tracks[track_id]\n",
    "            } for track_id, track in tracks.items()\n",
    "        }\n",
    "\n",
    "        scenario['map_features'] = {\n",
    "            feature_id: {\n",
    "                **feature,\n",
    "                'importance_score': map_scores[feature_id],\n",
    "                'token': tokenized_map_features[feature_id]\n",
    "            } for feature_id, feature in map_features.items()\n",
    "        }\n",
    "        \n",
    "        return scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Complete!\n",
      "Tracks Processed: 21\n",
      "Map Features Processed: 212\n",
      "Metadata: {'id': '0a14e451-2293-4a95-ab65-184427d890f3', 'coordinate': 'waymo', 'ts': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,\n",
      "        1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,\n",
      "        2.2,  2.3,  2.4,  2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,\n",
      "        3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,\n",
      "        4.4,  4.5,  4.6,  4.7,  4.8,  4.9,  5. ,  5.1,  5.2,  5.3,  5.4,\n",
      "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
      "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
      "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
      "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
      "        9.9, 10. , 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8, 10.9]), 'metadrive_processed': False, 'sdc_id': 'AV', 'dataset': 'av2', 'scenario_id': '0a14e451-2293-4a95-ab65-184427d890f3', 'source_file': '0a14e451-2293-4a95-ab65-184427d890f3', 'track_length': 110, 'current_time_index': 49, 'objects_of_interest': [], 'sdc_track_index': 20, 'tracks_to_predict': {'73980': {'track_index': 0, 'track_id': '73980', 'difficulty': 0, 'object_type': 'VEHICLE'}}, 'object_summary': {'73980': {'type': 'VEHICLE', 'object_id': '73980', 'track_length': 110, 'moving_distance': 83.07073974609375, 'valid_length': 110, 'continuous_valid_length': 110}, '74036': {'type': 'VEHICLE', 'object_id': '74036', 'track_length': 110, 'moving_distance': 3.012420415878296, 'valid_length': 33, 'continuous_valid_length': 33}, '74140': {'type': 'VEHICLE', 'object_id': '74140', 'track_length': 110, 'moving_distance': 0.10854719579219818, 'valid_length': 28, 'continuous_valid_length': 28}, '74157': {'type': 'VEHICLE', 'object_id': '74157', 'track_length': 110, 'moving_distance': 0.20654280483722687, 'valid_length': 21, 'continuous_valid_length': 21}, '74167': {'type': 'VEHICLE', 'object_id': '74167', 'track_length': 110, 'moving_distance': 0.1667182743549347, 'valid_length': 18, 'continuous_valid_length': 18}, '74225': {'type': 'VEHICLE', 'object_id': '74225', 'track_length': 110, 'moving_distance': 0.23332378268241882, 'valid_length': 19, 'continuous_valid_length': 19}, '74231': {'type': 'OTHER', 'object_id': '74231', 'track_length': 110, 'moving_distance': 2.7098162174224854, 'valid_length': 89, 'continuous_valid_length': 89}, '74250': {'type': 'OTHER', 'object_id': '74250', 'track_length': 110, 'moving_distance': 1.5489386320114136, 'valid_length': 17, 'continuous_valid_length': 17}, '74252': {'type': 'OTHER', 'object_id': '74252', 'track_length': 110, 'moving_distance': 2.6362509727478027, 'valid_length': 38, 'continuous_valid_length': 38}, '74253': {'type': 'VEHICLE', 'object_id': '74253', 'track_length': 110, 'moving_distance': 2.150221824645996, 'valid_length': 85, 'continuous_valid_length': 85}, '74254': {'type': 'VEHICLE', 'object_id': '74254', 'track_length': 110, 'moving_distance': 2.027728796005249, 'valid_length': 81, 'continuous_valid_length': 81}, '74256': {'type': 'VEHICLE', 'object_id': '74256', 'track_length': 110, 'moving_distance': 3.0412142276763916, 'valid_length': 75, 'continuous_valid_length': 75}, '74262': {'type': 'VEHICLE', 'object_id': '74262', 'track_length': 110, 'moving_distance': 4.149805545806885, 'valid_length': 11, 'continuous_valid_length': 11}, '74263': {'type': 'VEHICLE', 'object_id': '74263', 'track_length': 110, 'moving_distance': 43.59075927734375, 'valid_length': 65, 'continuous_valid_length': 65}, '74264': {'type': 'VEHICLE', 'object_id': '74264', 'track_length': 110, 'moving_distance': 50.82166290283203, 'valid_length': 33, 'continuous_valid_length': 33}, '74271': {'type': 'VEHICLE', 'object_id': '74271', 'track_length': 110, 'moving_distance': 16.355072021484375, 'valid_length': 41, 'continuous_valid_length': 41}, '74275': {'type': 'VEHICLE', 'object_id': '74275', 'track_length': 110, 'moving_distance': 16.828935623168945, 'valid_length': 16, 'continuous_valid_length': 16}, '74276': {'type': 'OTHER', 'object_id': '74276', 'track_length': 110, 'moving_distance': 0.9038136005401611, 'valid_length': 27, 'continuous_valid_length': 27}, '74286': {'type': 'VEHICLE', 'object_id': '74286', 'track_length': 110, 'moving_distance': 1.7012447118759155, 'valid_length': 22, 'continuous_valid_length': 22}, '74290': {'type': 'VEHICLE', 'object_id': '74290', 'track_length': 110, 'moving_distance': 15.241682052612305, 'valid_length': 18, 'continuous_valid_length': 18}, 'AV': {'type': 'VEHICLE', 'object_id': 'AV', 'track_length': 110, 'moving_distance': 71.94290161132812, 'valid_length': 110, 'continuous_valid_length': 110}}, 'number_summary': {'num_objects': 21, 'object_types': {'VEHICLE', 'OTHER'}, 'num_objects_each_type': {'VEHICLE': 17, 'OTHER': 4}, 'num_moving_objects': 16, 'num_moving_objects_each_type': defaultdict(<class 'int'>, {'VEHICLE': 13, 'OTHER': 3}), 'num_traffic_lights': 0, 'num_traffic_light_types': set(), 'num_traffic_light_each_step': {}, 'num_map_features': 212, 'map_height_diff': 4.300000190734863}}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AdaptiveTokenScenarioNet(token_length=10, use_gpu=True)\n",
    "tokens = tokenizer.process(scenario)\n",
    "\n",
    "print(\"Tokenization Complete!\")\n",
    "print(f\"Tracks Processed: {len(tokens['tracks'])}\")\n",
    "print(f\"Map Features Processed: {len(tokens['map_features'])}\")\n",
    "print(f\"Metadata: {tokens['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(345847.72)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['tracks']['73980']['importance_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

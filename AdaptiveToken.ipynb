{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scenarionet import read_dataset_summary, read_scenario\n",
    "from metadrive.engine.asset_loader import AssetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveTokenizer:\n",
    "    def __init__(self, importance_threshold=0.5, max_tokens=512, token_length=10):\n",
    "        self.importance_threshold = importance_threshold\n",
    "        self.max_tokens = max_tokens\n",
    "        self.token_length = token_length\n",
    "        self.type_importance = {\n",
    "            'VEHICLE': 1.0,\n",
    "            'PEDESTRIAN': 0.8,\n",
    "            'BICYCLE': 0.7,\n",
    "            'STATIC': 0.5,\n",
    "            'UNKNOWN': 0.3\n",
    "        }\n",
    "        self.map_feature_importance = {\n",
    "            'LANE': 0.9,\n",
    "            'CROSSWALK': 0.7,\n",
    "            'STOP_SIGN': 0.6,\n",
    "            'TRAFFIC_LIGHT': 0.8,\n",
    "            'UNKNOWN': 0.4\n",
    "        }\n",
    "\n",
    "    def score_importance(self, tracks, map_features):\n",
    "        importance_map = np.zeros(len(tracks) + len(map_features))\n",
    "        \n",
    "        for i, (track_id, track) in enumerate(tracks.items()):\n",
    "            state = track['state']\n",
    "            valid = state['valid']\n",
    "            if not valid.any():\n",
    "                importance_map[i] = 0\n",
    "                continue\n",
    "\n",
    "            valid_positions = state['position'][valid]\n",
    "            valid_velocities = state['velocity'][valid]\n",
    "            valid_headings = state['heading'][valid]\n",
    "            valid_lengths = state['length'][valid]\n",
    "            valid_widths = state['width'][valid]\n",
    "            valid_heights = state['height'][valid]\n",
    "            \n",
    "            activity_score = np.linalg.norm(valid_positions, axis=1).sum()\n",
    "            velocity_score = np.linalg.norm(valid_velocities, axis=1).sum()\n",
    "            heading_score = np.abs(valid_headings).sum()\n",
    "            size_score = valid_lengths.sum() + valid_widths.sum() + valid_heights.sum()\n",
    "            type_score = self.type_importance.get(track.get('type', 'UNKNOWN'), 0.3)\n",
    "            \n",
    "            importance_map[i] = activity_score + velocity_score + heading_score + size_score + type_score\n",
    "\n",
    "        for i, (feature_id, feature) in enumerate(map_features.items(), start=len(tracks)):\n",
    "            feature_type = feature.get('type', 'UNKNOWN')\n",
    "            polyline = feature.get('polyline', [])\n",
    "            feature_score = len(polyline) * self.map_feature_importance.get(feature_type, 0.4)\n",
    "            importance_map[i] = feature_score\n",
    "        \n",
    "        importance_map = importance_map / (importance_map.max() + 1e-5)\n",
    "        return importance_map\n",
    "\n",
    "    def pad_or_truncate(self, token):\n",
    "        if token.shape[0] > self.token_length:\n",
    "            return token[:self.token_length]\n",
    "        else:\n",
    "            pad_length = self.token_length - token.shape[0]\n",
    "            pad = np.zeros((pad_length, token.shape[1]), dtype=np.float32)\n",
    "            return np.vstack((token, pad))\n",
    "\n",
    "    def tokenize(self, tracks, map_features, metadata):\n",
    "        importance_scores = self.score_importance(tracks, map_features)\n",
    "        track_regions = []\n",
    "        map_regions = []\n",
    "        token_types = []\n",
    "        token_ids = []\n",
    "\n",
    "        for i, (track_id, track) in enumerate(tracks.items()):\n",
    "            state = track['state']\n",
    "            valid = state['valid']\n",
    "            if not valid.any():\n",
    "                continue\n",
    "            \n",
    "            valid_positions = state['position'][valid]\n",
    "            valid_velocities = state['velocity'][valid]\n",
    "            valid_headings = state['heading'][valid]\n",
    "            valid_lengths = state['length'][valid]\n",
    "            valid_widths = state['width'][valid]\n",
    "            valid_heights = state['height'][valid]\n",
    "            \n",
    "            combined_token = np.column_stack((valid_positions, valid_velocities, valid_headings, valid_lengths, valid_widths, valid_heights))\n",
    "            if importance_scores[i] >= self.importance_threshold:\n",
    "                token = self.pad_or_truncate(combined_token)\n",
    "                token_types.append(f'high-detail-{track.get(\"type\", \"UNKNOWN\")}')\n",
    "            else:\n",
    "                token = self.pad_or_truncate(combined_token[::2])\n",
    "                token_types.append(f'low-detail-{track.get(\"type\", \"UNKNOWN\")}')\n",
    "            \n",
    "            track_regions.append(token)\n",
    "            token_ids.append(track_id)\n",
    "\n",
    "        for i, (feature_id, feature) in enumerate(map_features.items(), start=len(tracks)):\n",
    "            polyline = feature.get('polyline', [])\n",
    "            if len(polyline) > 0:\n",
    "                polyline_array = np.array(polyline)\n",
    "                token = self.pad_or_truncate(polyline_array)\n",
    "                token_types.append(f'map-{feature.get(\"type\", \"UNKNOWN\")}')\n",
    "                map_regions.append(token)\n",
    "                token_ids.append(feature_id)\n",
    "\n",
    "        track_regions = track_regions[:self.max_tokens]\n",
    "        map_regions = map_regions[:self.max_tokens]\n",
    "        token_types = token_types[:self.max_tokens]\n",
    "        token_ids = token_ids[:self.max_tokens]\n",
    "\n",
    "        try:\n",
    "            track_tensor = torch.tensor(np.stack(track_regions), dtype=torch.float32) if track_regions else torch.empty(0)\n",
    "            map_tensor = torch.tensor(np.stack(map_regions), dtype=torch.float32) if map_regions else torch.empty(0)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Failed to stack token regions into tensors. Track shapes: {[t.shape for t in track_regions]}, Map shapes: {[m.shape for m in map_regions]}\") from e\n",
    "\n",
    "        return {\n",
    "            'track_regions': track_tensor,\n",
    "            'map_regions': map_tensor,\n",
    "            'token_types': token_types,\n",
    "            'token_ids': token_ids,\n",
    "            'metadata': metadata\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Complete!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'token_regions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sample_tracks, sample_map_features, sample_metadata)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_regions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken Types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_types\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token_regions'"
     ]
    }
   ],
   "source": [
    "sample_tracks = {\n",
    "        'track_1': {'state': {'position': np.random.rand(10, 3), 'velocity': np.random.rand(10, 2), 'heading': np.random.rand(10), \n",
    "                              'length': np.random.rand(10), 'width': np.random.rand(10), 'height': np.random.rand(10), \n",
    "                              'valid': np.array([True] * 10)}, 'type': 'VEHICLE'}\n",
    "    }\n",
    "sample_map_features = {\n",
    "        'lane_1': {'type': 'LANE', 'polyline': np.random.rand(5, 2)}\n",
    "    }\n",
    "sample_metadata = {'scenario_id': 'sample_001', 'map': 'city_map_1'}\n",
    "\n",
    "tokenizer = AdaptiveTokenizer(token_length=10)\n",
    "tokens = tokenizer.tokenize(sample_tracks, sample_map_features, sample_metadata)\n",
    "\n",
    "print(\"Tokenization Complete!\")\n",
    "print(f\"Track Tensor Shape: {tokens['track_regions'].shape}\")\n",
    "print(f\"Map Tensor Shape: {tokens['map_regions'].shape}\")\n",
    "print(f\"Token Types: {set(tokens['token_types'])}\")\n",
    "print(f\"Token IDs: {tokens['token_ids']}\")\n",
    "print(f\"Metadata: {tokens['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "av2_data =  AssetLoader.file_path(\"/home/light/Documents/Thesis/preprocessed_dataset\", unix_style=False)\n",
    "dataset_summary, scenario_ids, mapping = read_dataset_summary(dataset_path=av2_data)\n",
    "\n",
    "scenario_file_name = scenario_ids[0]\n",
    "scenario = read_scenario(dataset_path=av2_data, mapping=mapping, scenario_file_name=scenario_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'version', 'length', 'tracks', 'dynamic_map_states', 'map_features', 'metadata'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
